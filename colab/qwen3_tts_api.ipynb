{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Qwen3-TTS Voice Cloning API (Colab)\n\nRun on a free T4 GPU to serve Qwen3-TTS as an API for the voice app.\n\n**First time**: Run cells 1-5 to create and save your voice clone prompt to Google Drive.\n\n**Each session**: Run cells 1-4, then cell 6 to start the API server.\n\n**Sharing**: Send this notebook link â€” each person uses their own reference audio + Google Drive."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 1: Install dependencies\n!pip install -q qwen-tts pyngrok fastapi uvicorn soundfile",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 2: Check GPU\nimport torch\nprint(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'NONE'}\")\nif torch.cuda.is_available():\n    props = torch.cuda.get_device_properties(0)\n    vram = getattr(props, 'total_memory', None) or getattr(props, 'total_mem', 0)\n    print(f\"VRAM: {vram / 1e9:.1f} GB\")\nassert torch.cuda.is_available(), \"GPU required! Go to Runtime > Change runtime type > T4 GPU\"",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 3: (Optional) Install flash-attn â€” only works on Ampere+ GPUs (A100, L4, etc.)\n# T4 GPUs do NOT support flash-attn. Skip this cell on T4.\n# !pip install -q flash-attn --no-build-isolation\nprint(\"Skipping flash-attn (not supported on T4). Using default attention.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 4: Load model\nimport torch\nfrom qwen_tts import Qwen3TTSModel\n\nmodel = Qwen3TTSModel.from_pretrained(\n    \"Qwen/Qwen3-TTS-12Hz-1.7B-Base\",\n    device_map=\"cuda:0\",\n    dtype=torch.bfloat16,\n)\nprint(\"Model loaded!\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 5: Extract & save voice clone prompt (RUN ONCE)\n# Upload your reference audio when prompted, then it saves to Google Drive.\n# You also need to provide the transcript of what's said in the reference audio.\n\nfrom google.colab import drive, files\nimport os\nimport pickle\n\ndrive.mount('/content/drive')\nos.makedirs('/content/drive/MyDrive/qwen3-tts', exist_ok=True)\n\nprint(\"Upload your reference audio file (WAV/MP3, 10-30 seconds of clear speech):\")\nuploaded = files.upload()\nref_path = list(uploaded.keys())[0]\nprint(f\"Using: {ref_path}\")\n\nref_text = input(\"Enter the transcript of the reference audio: \")\n\n# Create voice clone prompt (includes x-vector + acoustic info)\n# Set x_vector_only_mode=True if you don't want to provide ref_text (lower quality)\nvoice_clone_prompt = model.create_voice_clone_prompt(\n    ref_audio=ref_path,\n    ref_text=ref_text,\n    x_vector_only_mode=False,\n)\n\nsave_path = '/content/drive/MyDrive/qwen3-tts/voice_clone_prompt.pkl'\nwith open(save_path, 'wb') as f:\n    pickle.dump(voice_clone_prompt, f)\nprint(f\"Voice clone prompt saved to {save_path}\")\nprint(\"You won't need to run this cell again unless you want a different voice.\")",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Cell 6: Start TTS API server\n# Run this each session after cells 1-4.\n# SETUP: Add your ngrok token as a Colab Secret named NGROK_TOKEN\n#   Click the ðŸ”‘ (Secrets) icon in the left sidebar â†’ Add new secret â†’ Name: NGROK_TOKEN\n\nfrom google.colab import drive, userdata\nimport torch\nimport os\nimport io\nimport pickle\nimport soundfile as sf\nfrom fastapi import FastAPI, HTTPException\nfrom fastapi.responses import Response\nfrom pydantic import BaseModel\nfrom pyngrok import ngrok\nimport uvicorn\nimport threading\n\n# Mount drive and load voice clone prompt\ndrive.mount('/content/drive', force_remount=False)\nprompt_path = '/content/drive/MyDrive/qwen3-tts/voice_clone_prompt.pkl'\nassert os.path.exists(prompt_path), f\"Voice clone prompt not found at {prompt_path}. Run Cell 5 first!\"\nwith open(prompt_path, 'rb') as f:\n    voice_clone_prompt = pickle.load(f)\nprint(\"Loaded voice clone prompt from Drive\")\n\napp = FastAPI()\n\nclass TTSRequest(BaseModel):\n    text: str\n    language: str = \"English\"\n\n@app.post(\"/tts\")\nasync def tts(req: TTSRequest):\n    if not req.text.strip():\n        raise HTTPException(400, \"Empty text\")\n    try:\n        with torch.no_grad():\n            wavs, sr = model.generate_voice_clone(\n                text=req.text,\n                language=req.language,\n                voice_clone_prompt=voice_clone_prompt,\n            )\n        # Convert to WAV bytes\n        buf = io.BytesIO()\n        sf.write(buf, wavs[0], sr, format='WAV', subtype='PCM_16')\n        return Response(content=buf.getvalue(), media_type=\"audio/wav\")\n    except Exception as e:\n        raise HTTPException(500, str(e))\n\n@app.get(\"/health\")\nasync def health():\n    return {\"status\": \"ok\"}\n\n# Set ngrok auth token from Colab Secrets\nngrok.set_auth_token(userdata.get('NGROK_TOKEN'))\n\npublic_url = ngrok.connect(8000)\nprint(f\"\\n{'='*60}\")\nprint(f\"  TTS API is live!\")\nprint(f\"  Public URL: {public_url}\")\nprint(f\"  Paste this URL into the voice app's Qwen3 URL field.\")\nprint(f\"{'='*60}\\n\")\n\n# Run server in background thread\nthreading.Thread(\n    target=uvicorn.run,\n    args=(app,),\n    kwargs={\"host\": \"0.0.0.0\", \"port\": 8000, \"log_level\": \"info\"},\n    daemon=True\n).start()\n\nprint(\"Server running. Keep this cell running!\")\n# Keep cell alive\nimport time\nwhile True:\n    time.sleep(60)",
   "execution_count": null,
   "outputs": []
  }
 ]
}